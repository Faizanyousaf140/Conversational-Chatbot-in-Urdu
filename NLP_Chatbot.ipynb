{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-4KEyKYMaMn",
        "outputId": "074fb312-ec6a-4e87-b479-f1c95d7cf7c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:288: SyntaxWarning: invalid escape sequence '\\P'\n",
            "<>:288: SyntaxWarning: invalid escape sequence '\\P'\n",
            "C:\\Users\\G T\\AppData\\Local\\Temp\\ipykernel_9004\\2005792912.py:288: SyntaxWarning: invalid escape sequence '\\P'\n",
            "  print(\"\\Phase 1 Preprocessing Complete!\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded successfully!\n",
            "Total rows: 20000\n",
            "Columns: ['client_id', 'path', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accents', 'variant', 'locale', 'segment']\n",
            "\n",
            "First few sentences:\n",
            "0                   کبھی کبھار ہی خیالی پلاو بناتا ہوں\n",
            "1                    اور پھر ممکن ہے کہ پاکستان بھی ہو\n",
            "2                        یہ فیصلہ بھی گزشتہ دو سال میں\n",
            "3                       ان کے بلے بازوں کے سامنے ہو گا\n",
            "4    آبی جانور میں بطخ بگلا اور دُوسْرا آبی پرندہ ش...\n",
            "Name: sentence, dtype: object\n",
            "Original:   کبھی کبھار ہی خیالی پلاو بناتا ہوں\n",
            "Normalized: کبہی کبہار ہی خیالی پلاو بناتا ہوں\n",
            "\n",
            "Normalization function ready!\n",
            "Normalization complete!\n",
            "Total sentences after cleaning: 20000\n",
            "\n",
            "Sample normalized sentences:\n",
            "1. کبہی کبہار ہی خیالی پلاو بناتا ہوں\n",
            "2. اور پہر ممکن ہے کہ پاکستان بہی ہو\n",
            "3. یہ فیصلہ بہی گزشتہ دو سال میں\n",
            "4. ان کے بلے بازوں کے سامنے ہو گا\n",
            "5. ابی جانور میں بطخ بگلا اور دوسرا ابی پرندہ شامل ہونا\n",
            "Saved 20000 normalized sentences to 'urdu_sentences.txt'\n",
            "Training SentencePiece Unigram tokenizer...\n",
            "Unique characters in dataset: 109\n",
            "Setting vocabulary size to: 6000\n",
            "Training tokenizer...\n",
            "\n",
            "Tokenizer trained successfully!\n",
            "Vocabulary size: 6000\n",
            "Files created: urdu_tokenizer.model, urdu_tokenizer.vocab\n",
            "Vocabulary size: 6000\n",
            "\n",
            "--- Testing Tokenizer ---\n",
            "\n",
            "Original:  کبھی کبھار ہی خیالی پلاو بناتا ہوں\n",
            "Tokens:    ['▁کب', 'ھ', 'ی', '▁کب', 'ھ', 'ار', '▁ہی', '▁خیال', 'ی', '▁پلا', 'و', '▁بنات', 'ا', '▁ہوں']\n",
            "Token IDs: [521, 4093, 6, 521, 4093, 655, 40, 210, 6, 2054]...\n",
            "Decoded:   کبھی کبھار ہی خیالی پلاو بناتا ہوں\n",
            "\n",
            "Original:  آج موسم بہت اچھا ہے\n",
            "Tokens:    ['▁', 'آ', 'ج', '▁موسم', '▁بہت', '▁اچ', 'ھ', 'ا', '▁ہے']\n",
            "Token IDs: [26, 1, 168, 498, 59, 1148, 4093, 24, 12]...\n",
            "Decoded:   آج موسم بہت اچھا ہے\n",
            "\n",
            "Original:  بولو کیسے ہو\n",
            "Tokens:    ['▁بولو', '▁کیسے', '▁ہو']\n",
            "Token IDs: [5359, 266, 29]...\n",
            "Decoded:   بولو کیسے ہو\n",
            "\n",
            "Tokenizer working correctly!\n",
            "Tokenizing all sentences...\n",
            " All sentences tokenized!\n",
            "\n",
            "Token Statistics\n",
            "Average tokens per sentence: 10.04\n",
            "Min tokens: 1\n",
            "Max tokens: 1126\n",
            "Median tokens: 9.00\n",
            "\n",
            "Token Length Distribution \n",
            "count    20000.000000\n",
            "mean        10.043700\n",
            "std          9.228865\n",
            "min          1.000000\n",
            "25%          6.000000\n",
            "50%          9.000000\n",
            "75%         13.000000\n",
            "max       1126.000000\n",
            "Name: token_count, dtype: float64\n",
            "Splitting dataset...\n",
            " Dataset split complete!\n",
            "\n",
            "Split Statistics\n",
            "Training set:   16000 sentences (80.0%)\n",
            "Validation set: 2000 sentences (10.0%)\n",
            "Test set:       2000 sentences (10.0%)\n",
            "Total:          20000 sentences\n",
            "Saving processed datasets...\n",
            " Saved datasets:\n",
            "  train_data.csv\n",
            "  val_data.csv\n",
            "  test_data.csv\n",
            "Saved tokenized versions (pickle files)\n",
            "PREPROCESSING COMPLETE - SUMMARY\n",
            "\n",
            "Dataset Statistics:\n",
            "   Total sentences: 20000\n",
            "   Train: 16000 | Val: 2000 | Test: 2000\n",
            "\n",
            "Tokenizer:\n",
            "   Vocabulary size: 6000\n",
            "   Model type: Unigram LM\n",
            "   Special tokens: <pad>, <unk>, <s>, </s>, [MASK]\n",
            "\n",
            " Files Created:\n",
            "   ✓ urdu_tokenizer.model\n",
            "   ✓ urdu_tokenizer.vocab\n",
            "   ✓ train_data.csv\n",
            "   ✓ val_data.csv\n",
            "   ✓ test_data.csv\n",
            "   ✓ train_tokenized.pkl\n",
            "   ✓ val_tokenized.pkl\n",
            "   ✓ test_tokenized.pkl\n",
            "\n",
            " Sample from Training Set:\n",
            "\n",
            "   Sentence: انہوں نے یہ سفر اپنے شوہر کے ساتہ کیا ہے\n",
            "   Tokens: ['▁انہوں', '▁نے', '▁یہ', '▁سفر', '▁اپنے', '▁شو', 'ہر', '▁کے']...\n",
            "   Token count: 11\n",
            "\n",
            "   Sentence: سوڈییم این سے ب\n",
            "   Tokens: ['▁سوڈی', 'یم', '▁این', '▁سے', '▁ب']...\n",
            "   Token count: 5\n",
            "\n",
            "   Sentence: جن کا کام کچہ اور ہے۔\n",
            "   Tokens: ['▁جن', '▁کا', '▁کام', '▁کچہ', '▁اور', '▁ہے۔']...\n",
            "   Token count: 6\n",
            "\\Phase 1 Preprocessing Complete!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import sentencepiece as spm\n",
        "import os\n",
        "# Read the dataset\n",
        "df = pd.read_csv('final_main_dataset.tsv', sep='\\t', encoding='utf-8')\n",
        "\n",
        "print(f\"Dataset loaded successfully!\")\n",
        "\n",
        "# Breif Eda on Dataset\n",
        "print(f\"Total rows: {len(df)}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(\"\\nFirst few sentences:\")\n",
        "print(df['sentence'].head())\n",
        "\n",
        "def normalize_urdu_text(text):\n",
        "\n",
        "    if pd.isna(text):  # Handle NaN values\n",
        "        return \"\"\n",
        "\n",
        "    text = str(text)\n",
        "\n",
        "    # remove common urdu diacritics ie standardize alef and yeh forms(zabar, zer, pesh, etc.)\n",
        "    # Some most used and common standardized\n",
        "    diacritics = [\n",
        "        '\\u064B',\n",
        "        '\\u064C',\n",
        "        '\\u064D',\n",
        "        '\\u064E',\n",
        "        '\\u064F',\n",
        "        '\\u0650',\n",
        "        '\\u0651',\n",
        "        '\\u0652',\n",
        "        '\\u0653',\n",
        "        '\\u0654',\n",
        "        '\\u0655',\n",
        "        '\\u0656',\n",
        "        '\\u0657',\n",
        "        '\\u0658',\n",
        "    ]\n",
        "\n",
        "    for diacritic in diacritics:\n",
        "        text = text.replace(diacritic, '')\n",
        "\n",
        "    # Standardize different forms of alef\n",
        "    text = text.replace('أ', 'ا')\n",
        "    text = text.replace('إ', 'ا')\n",
        "    text = text.replace('آ', 'ا')\n",
        "    text = text.replace('ٱ', 'ا')\n",
        "\n",
        "    # Standardize different forms of yeh\n",
        "    text = text.replace('ی', 'ی')\n",
        "    text = text.replace('ے', 'ے')\n",
        "    text = text.replace('ئ', 'ی')\n",
        "\n",
        "    # Standardize heh forms\n",
        "    text = text.replace('ۃ', 'ہ')\n",
        "    text = text.replace('ھ', 'ہ')\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# Test the function\n",
        "test_sentence = \"کبھی کبھار ہی خیالی پلاو بناتا ہوں\"\n",
        "normalized = normalize_urdu_text(test_sentence)\n",
        "print(f\"Original:   {test_sentence}\")\n",
        "print(f\"Normalized: {normalized}\")\n",
        "print(\"\\nNormalization function ready!\")\n",
        "\n",
        "\n",
        "# Apply Normalization to All Sentences\n",
        "\n",
        "df['normalized_sentence'] = df['sentence'].apply(normalize_urdu_text)\n",
        "\n",
        "# Remove empty sentences after normalization\n",
        "df = df[df['normalized_sentence'].str.len() > 0].reset_index(drop=True)\n",
        "\n",
        "print(f\"Normalization complete!\")\n",
        "\n",
        "print(f\"Total sentences after cleaning: {len(df)}\")\n",
        "print(\"\\nSample normalized sentences:\")\n",
        "for i in range(5):\n",
        "    print(f\"{i+1}. {df['normalized_sentence'].iloc[i]}\")\n",
        "\n",
        "\n",
        "# Save Normalized Sentences to Text File for tokenizer training\n",
        "\n",
        "with open('urdu_sentences.txt', 'w', encoding='utf-8') as f:\n",
        "    for sentence in df['normalized_sentence']:\n",
        "        f.write(sentence + '\\n')\n",
        "print(f\"Saved {len(df)} normalized sentences to 'urdu_sentences.txt'\")\n",
        "\n",
        "# Train SentencePiece Tokenizer with Automatic Vocab Size\n",
        "\n",
        "print(\"Training SentencePiece Unigram tokenizer...\")\n",
        "\n",
        "# Checking unique characters\n",
        "with open('urdu_sentences.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "    unique_chars = len(set(text))\n",
        "\n",
        "print(f\"Unique characters in dataset: {unique_chars}\")\n",
        "\n",
        "# Set Reasonable vocab size based on dataset size\n",
        "total_sentences = len(df)\n",
        "\n",
        "if total_sentences < 5000:\n",
        "    vocab_size = 3000\n",
        "elif total_sentences < 10000:\n",
        "    vocab_size = 4000\n",
        "elif total_sentences < 15000:\n",
        "    vocab_size = 5000\n",
        "else:\n",
        "    vocab_size = 6000\n",
        "\n",
        "print(f\"Setting vocabulary size to: {vocab_size}\")\n",
        "print(\"Training tokenizer...\\n\")\n",
        "\n",
        "# Train the tokenizer with adjusted vocab size\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input='urdu_sentences.txt',\n",
        "    model_prefix='urdu_tokenizer',\n",
        "    vocab_size=vocab_size,\n",
        "    character_coverage=1.0,  # Cover all Urdu characters\n",
        "    model_type='unigram',  # Unigram Language Model\n",
        "    pad_id=0,  # Padding token\n",
        "    unk_id=1,  # Unknown token\n",
        "    bos_id=2,  # Begin of sentence\n",
        "    eos_id=3,  # End of sentence\n",
        "    user_defined_symbols=['[MASK]'],  # Masking special tokens\n",
        "    num_threads=4,\n",
        ")\n",
        "\n",
        "print(\"Tokenizer trained successfully!\")\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(\"Files created: urdu_tokenizer.model, urdu_tokenizer.vocab\")\n",
        "\n",
        "\n",
        "# Load the trained tokenizer\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('urdu_tokenizer.model')\n",
        "print(f\"Vocabulary size: {sp.vocab_size()}\")\n",
        "print(f\"\\n--- Testing Tokenizer ---\")\n",
        "\n",
        "# Test sentences\n",
        "test_sentences = [\n",
        "    \"کبھی کبھار ہی خیالی پلاو بناتا ہوں\",\n",
        "    \"آج موسم بہت اچھا ہے\",\n",
        "    \"بولو کیسے ہو\"\n",
        "]\n",
        "\n",
        "for sent in test_sentences:\n",
        "    tokens = sp.encode_as_pieces(sent)\n",
        "    ids = sp.encode_as_ids(sent)\n",
        "\n",
        "    print(f\"\\nOriginal:  {sent}\")\n",
        "    print(f\"Tokens:    {tokens}\")\n",
        "    print(f\"Token IDs: {ids[:10]}...\")  # Show first 10 IDs\n",
        "\n",
        "    # Decode back\n",
        "    decoded = sp.decode_pieces(tokens)\n",
        "    print(f\"Decoded:   {decoded}\")\n",
        "\n",
        "print(\"\\nTokenizer working correctly!\")\n",
        "\n",
        "# Tokenize All Sentences\n",
        "\n",
        "print(\"Tokenizing all sentences...\")\n",
        "\n",
        "# Tokenize all normalized sentences\n",
        "df['tokenized'] = df['normalized_sentence'].apply(lambda x: sp.encode_as_ids(x))\n",
        "df['token_count'] = df['tokenized'].apply(len)\n",
        "\n",
        "print(f\" All sentences tokenized!\")\n",
        "print(f\"\\nToken Statistics\")\n",
        "print(f\"Average tokens per sentence: {df['token_count'].mean():.2f}\")\n",
        "print(f\"Min tokens: {df['token_count'].min()}\")\n",
        "print(f\"Max tokens: {df['token_count'].max()}\")\n",
        "print(f\"Median tokens: {df['token_count'].median():.2f}\")\n",
        "\n",
        "# Show distribution\n",
        "print(f\"\\nToken Length Distribution \")\n",
        "print(df['token_count'].describe())\n",
        "\n",
        "#  Split Dataset (80% Train, 10% Val, 10% Test)\n",
        "\n",
        "print(\"Splitting dataset...\")\n",
        "\n",
        "# First split: 80% train, 20% temp\n",
        "train_df, temp_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Second split: 10% val, 10% test (from the 20% temp)\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df,\n",
        "    test_size=0.5,\n",
        "    random_state=42,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Reset indices\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "print(f\" Dataset split complete!\")\n",
        "print(f\"\\nSplit Statistics\")\n",
        "print(f\"Training set:   {len(train_df)} sentences ({len(train_df)/len(df)*100:.1f}%)\")\n",
        "print(f\"Validation set: {len(val_df)} sentences ({len(val_df)/len(df)*100:.1f}%)\")\n",
        "print(f\"Test set:       {len(test_df)} sentences ({len(test_df)/len(df)*100:.1f}%)\")\n",
        "print(f\"Total:          {len(df)} sentences\")\n",
        "\n",
        "#  Save Processed Datasets\n",
        "\n",
        "print(\"Saving processed datasets...\")\n",
        "\n",
        "# Save to CSV files\n",
        "train_df.to_csv('train_data.csv', index=False)\n",
        "val_df.to_csv('val_data.csv', index=False)\n",
        "test_df.to_csv('test_data.csv', index=False)\n",
        "\n",
        "print(\" Saved datasets:\")\n",
        "print(\"  train_data.csv\")\n",
        "print(\"  val_data.csv\")\n",
        "print(\"  test_data.csv\")\n",
        "\n",
        "# Save tokenized versions for further use\n",
        "import pickle\n",
        "\n",
        "with open('train_tokenized.pkl', 'wb') as f:\n",
        "    pickle.dump(train_df['tokenized'].tolist(), f)\n",
        "\n",
        "with open('val_tokenized.pkl', 'wb') as f:\n",
        "    pickle.dump(val_df['tokenized'].tolist(), f)\n",
        "\n",
        "with open('test_tokenized.pkl', 'wb') as f:\n",
        "    pickle.dump(test_df['tokenized'].tolist(), f)\n",
        "\n",
        "print(\"Saved tokenized versions (pickle files)\")\n",
        "\n",
        "\n",
        "# Summary and Verification\n",
        "\n",
        "print(\"PREPROCESSING COMPLETE - SUMMARY\")\n",
        "\n",
        "\n",
        "print(f\"\\nDataset Statistics:\")\n",
        "print(f\"   Total sentences: {len(df)}\")\n",
        "print(f\"   Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n",
        "\n",
        "print(f\"\\nTokenizer:\")\n",
        "print(f\"   Vocabulary size: {sp.vocab_size()}\")\n",
        "print(f\"   Model type: Unigram LM\")\n",
        "print(f\"   Special tokens: <pad>, <unk>, <s>, </s>, [MASK]\")\n",
        "\n",
        "print(f\"\\n Files Created:\")\n",
        "files = [\n",
        "    'urdu_tokenizer.model',\n",
        "    'urdu_tokenizer.vocab',\n",
        "    'train_data.csv',\n",
        "    'val_data.csv',\n",
        "    'test_data.csv',\n",
        "    'train_tokenized.pkl',\n",
        "    'val_tokenized.pkl',\n",
        "    'test_tokenized.pkl'\n",
        "]\n",
        "\n",
        "for file in files:\n",
        "    exists = \"✓\" if os.path.exists(file) else \"✗\"\n",
        "    print(f\"   {exists} {file}\")\n",
        "\n",
        "print(f\"\\n Sample from Training Set:\")\n",
        "sample = train_df.sample(3)\n",
        "for idx, row in sample.iterrows():\n",
        "    print(f\"\\n   Sentence: {row['normalized_sentence']}\")\n",
        "    print(f\"   Tokens: {sp.encode_as_pieces(row['normalized_sentence'])[:8]}...\")\n",
        "    print(f\"   Token count: {row['token_count']}\")\n",
        "\n",
        "print(\"\\Phase 1 Preprocessing Complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h_ShyudcOVxO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # Add batch dimension\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, d_model)\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads # it is the dimension of each head key, query and value vectors\n",
        "\n",
        "        # Each token embedding divided into query, key and value vectors\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # Linear projections\n",
        "        Q = self.W_q(query)\n",
        "        K = self.W_k(key)\n",
        "        V = self.W_v(value)\n",
        "\n",
        "       # Project and split into heads: (B, L, d_model) -> (B, h, L, d_head)\n",
        "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # compute attention scores b/w every token pair\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k) # divide by sqrt(d_k) to prevent large values\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(attn, V)\n",
        "         # Concatenate heads: (B, h, L, d_head) -> (B, L, d_model)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        return self.W_o(output)\n",
        "\n",
        "class FeedForward(nn.Module):   # after applying attention doing some non-linear transformation to learn more contextual patterns\n",
        "    def __init__(self, d_model, d_ff=1024, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(torch.relu(self.linear1(x))))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        #  Apply Self-attention\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "\n",
        "        # Apply Feed-forward\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "\n",
        "        return x\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
        "        # Self-attention\n",
        "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "\n",
        "        # Cross-attention with encoder output\n",
        "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(attn_output))\n",
        "\n",
        "        # Feed-forward\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, num_heads=2,\n",
        "                 num_encoder_layers=2, num_decoder_layers=2,\n",
        "                 d_ff=1024, dropout=0.1, max_len=512):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Add Embeddings and positional encoding\n",
        "        self.encoder_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        # Encoder layers\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_encoder_layers)\n",
        "        ])\n",
        "\n",
        "        # Decoder layers\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_decoder_layers)\n",
        "        ])\n",
        "\n",
        "        # Output projection by mapping decoder output back to vocab size\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        # Encode\n",
        "        src_emb = self.dropout(self.positional_encoding(\n",
        "            self.encoder_embedding(src) * math.sqrt(self.d_model)))\n",
        "\n",
        "        enc_output = src_emb\n",
        "        for layer in self.encoder_layers:\n",
        "            enc_output = layer(enc_output, src_mask)\n",
        "\n",
        "        # Decode\n",
        "        tgt_emb = self.dropout(self.positional_encoding(\n",
        "            self.decoder_embedding(tgt) * math.sqrt(self.d_model)))\n",
        "\n",
        "        dec_output = tgt_emb\n",
        "        for layer in self.decoder_layers:\n",
        "            dec_output = layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        # Output\n",
        "        output = self.fc_out(dec_output)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7H2CUKh5B2Us",
        "outputId": "5c35ee51-2c64-496e-8f54-c5213bf21c84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized!\n",
            "Device: cpu\n",
            "Vocabulary size: 6000\n",
            "Total parameters: 8,300,400\n",
            "Trainable parameters: 8,300,400\n",
            "Model forward pass successful!\n",
            "Input shape (src): torch.Size([2, 10])\n",
            "Input shape (tgt): torch.Size([2, 8])\n",
            "Output shape: torch.Size([2, 8, 6000])\n",
            "Expected: (2, 8, 6000)\n"
          ]
        }
      ],
      "source": [
        "import sentencepiece as spm\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('urdu_tokenizer.model')\n",
        "\n",
        "vocab_size = sp.vocab_size()\n",
        "# Create the model\n",
        "model = Transformer(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=256,\n",
        "    num_heads=2,\n",
        "    num_encoder_layers=2,\n",
        "    num_decoder_layers=2,\n",
        "    d_ff=1024,\n",
        "    dropout=0.1,\n",
        "    max_len=512\n",
        ")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "# Count total parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Model initialized!\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "# Making a dummy input\n",
        "batch_size = 2\n",
        "src_len = 10\n",
        "tgt_len = 8\n",
        "\n",
        "src = torch.randint(0, vocab_size, (batch_size, src_len)).to(device)\n",
        "tgt = torch.randint(0, vocab_size, (batch_size, tgt_len)).to(device)\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    output = model(src, tgt)\n",
        "print(\"Model forward pass successful!\")\n",
        "print(f\"Input shape (src): {src.shape}\")\n",
        "print(f\"Input shape (tgt): {tgt.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Expected: ({batch_size}, {tgt_len}, {vocab_size})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3ZcpxPdLWP4P"
      },
      "outputs": [],
      "source": [
        "# Span Corruption + Masked Data Creation\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "MASK_TOKEN = \"[MASK]\"\n",
        "PAD_TOKEN = \"[PAD]\"\n",
        "UNK_TOKEN = \"[UNK]\"\n",
        "SOS_TOKEN = \"<sos>\"\n",
        "EOS_TOKEN = \"<eos>\"\n",
        "\n",
        "MASK_PROB = 0.15  # 15% tokens will be masked\n",
        "RANDOM_TOKEN_PROB = 0.10  # 10% of masked tokens replaced with random token\n",
        "UNCHANGED_PROB = 0.10      # 10% of masked tokens stay same\n",
        "\n",
        "\n",
        "def apply_span_corruption(tokens, vocab, mask_prob=MASK_PROB):\n",
        "\n",
        "   # Apply T5-style span corruption + BERT-style 15% masking rule.\n",
        "    output_tokens = tokens.copy()\n",
        "    target_tokens = []\n",
        "    num_to_mask = max(1, int(len(tokens) * mask_prob)) # number of tokens to mask (atleast 1)\n",
        "\n",
        "    # Randomly select spans (each span length 1–3)\n",
        "    mask_indices = set()\n",
        "    i = 0\n",
        "    while len(mask_indices) < num_to_mask and i < len(tokens):\n",
        "        span_len = random.choice([1, 2, 3])\n",
        "        start = random.randint(0, len(tokens) - span_len)\n",
        "        for j in range(start, min(start + span_len, len(tokens))):\n",
        "            mask_indices.add(j)\n",
        "        i += 1\n",
        "\n",
        "    # Apply 80/10/10 masking rule\n",
        "    for idx in range(len(tokens)):\n",
        "        if idx in mask_indices:\n",
        "            rand = random.random()\n",
        "            if rand < 0.8:\n",
        "                output_tokens[idx] = MASK_TOKEN\n",
        "            elif rand < 0.9:\n",
        "                output_tokens[idx] = random.choice(list(vocab.keys()))\n",
        "            else:\n",
        "                pass  # keep unchanged\n",
        "            target_tokens.append(tokens[idx])\n",
        "        else:\n",
        "            target_tokens.append(PAD_TOKEN)\n",
        "\n",
        "    return output_tokens, target_tokens\n",
        "\n",
        "\n",
        "class MaskedUrduDataset(Dataset):\n",
        "    def __init__(self, df, vocab, tokenizer, max_len=50):\n",
        "        self.df = df\n",
        "        self.vocab = vocab\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_text = str(self.df.iloc[idx]['input_text'])\n",
        "        tgt_text = str(self.df.iloc[idx]['target_text'])\n",
        "\n",
        "        # Tokenize input (apply corruption)\n",
        "        tokens = self.tokenizer(src_text)\n",
        "        corrupted, target = apply_span_corruption(tokens, self.vocab)\n",
        "\n",
        "        # Convert to IDs\n",
        "        src_ids = [self.vocab.get(tok, self.vocab[UNK_TOKEN]) for tok in corrupted]\n",
        "        tgt_ids = [self.vocab.get(tok, self.vocab[UNK_TOKEN]) for tok in target]\n",
        "\n",
        "        # Add SOS and EOS\n",
        "        src_ids = [self.vocab[SOS_TOKEN]] + src_ids[:self.max_len-2] + [self.vocab[EOS_TOKEN]]\n",
        "        tgt_ids = [self.vocab[SOS_TOKEN]] + tgt_ids[:self.max_len-2] + [self.vocab[EOS_TOKEN]]\n",
        "\n",
        "        # Pad\n",
        "        src_ids += [self.vocab[PAD_TOKEN]] * (self.max_len - len(src_ids))\n",
        "        tgt_ids += [self.vocab[PAD_TOKEN]] * (self.max_len - len(tgt_ids))\n",
        "\n",
        "        return torch.tensor(src_ids), torch.tensor(tgt_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-K1oxtGaPS8p",
        "outputId": "8f2a4f83-175a-4a45-f310-eb16279ff0ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded successfully!\n",
            "Train: 16000, Val: 2000, Test: 2000\n",
            "Masked Datasets ready!\n",
            "Train: 15713 | Val: 1966 | Test: 1981\n",
            "Training setup complete on cpu\n",
            "⏭ Training skipped. Loading pretrained model instead...\n",
            "Model loaded successfully and ready for inference!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import sentencepiece as spm\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ----------------------------\n",
        "# Load tokenizer\n",
        "# ----------------------------\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(\"urdu_tokenizer.model\")\n",
        "\n",
        "# ----------------------------\n",
        "# Load pre-tokenized data\n",
        "# ----------------------------\n",
        "with open(\"train_tokenized.pkl\", \"rb\") as f:\n",
        "    train_data = pickle.load(f)\n",
        "with open(\"val_tokenized.pkl\", \"rb\") as f:\n",
        "    val_data = pickle.load(f)\n",
        "with open(\"test_tokenized.pkl\", \"rb\") as f:\n",
        "    test_data = pickle.load(f)\n",
        "\n",
        "print(\"Data loaded successfully!\")\n",
        "print(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Span corruption configuration\n",
        "# ----------------------------\n",
        "MASK_ID = sp.piece_to_id(\"<mask>\") if sp.piece_to_id(\"<mask>\") != 0 else 1\n",
        "PAD_ID = 0\n",
        "SOS_ID = 2\n",
        "EOS_ID = 3\n",
        "\n",
        "def apply_span_corruption(token_ids, mask_prob=0.15):\n",
        "    output_tokens = token_ids.copy()\n",
        "    target_tokens = token_ids.copy()\n",
        "    num_to_mask = max(1, int(len(token_ids) * mask_prob))\n",
        "\n",
        "    mask_indices = set()\n",
        "    while len(mask_indices) < num_to_mask:\n",
        "        span_len = random.choice([1, 2, 3])\n",
        "        start = random.randint(0, len(token_ids) - span_len)\n",
        "        for j in range(start, min(start + span_len, len(token_ids))):\n",
        "            mask_indices.add(j)\n",
        "        if len(mask_indices) >= num_to_mask:\n",
        "            break\n",
        "\n",
        "    for idx in range(len(token_ids)):\n",
        "        if idx in mask_indices:\n",
        "            r = random.random()\n",
        "            if r < 0.8:\n",
        "                output_tokens[idx] = MASK_ID      # 80% mask\n",
        "            elif r < 0.9:\n",
        "                output_tokens[idx] = random.randint(4, sp.get_piece_size() - 1)  # 10% random\n",
        "            # 10% unchanged\n",
        "\n",
        "    return output_tokens, target_tokens\n",
        "\n",
        "# ----------------------------\n",
        "# Masked dataset\n",
        "# ----------------------------\n",
        "class MaskedUrduDataset(Dataset):\n",
        "    def __init__(self, tokenized_sentences, max_len=50):\n",
        "        self.data = []\n",
        "        self.max_len = max_len\n",
        "\n",
        "        for tokens in tokenized_sentences:\n",
        "            if len(tokens) < 3:\n",
        "                continue\n",
        "\n",
        "            corrupted, target = apply_span_corruption(tokens)\n",
        "\n",
        "            src = [SOS_ID] + corrupted[:max_len-2] + [EOS_ID]\n",
        "            tgt = [SOS_ID] + target[:max_len-2] + [EOS_ID]\n",
        "\n",
        "            src += [PAD_ID] * (max_len - len(src))\n",
        "            tgt += [PAD_ID] * (max_len - len(tgt))\n",
        "\n",
        "            self.data.append((src, tgt))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.data[idx][0]), torch.tensor(self.data[idx][1])\n",
        "\n",
        "\n",
        "train_dataset = MaskedUrduDataset(train_data)\n",
        "val_dataset = MaskedUrduDataset(val_data)\n",
        "test_dataset = MaskedUrduDataset(test_data)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(\"Masked Datasets ready!\")\n",
        "print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Masks for Transformer\n",
        "# ----------------------------\n",
        "def create_masks(src, tgt):\n",
        "    src_mask = (src != PAD_ID).unsqueeze(1).unsqueeze(2)\n",
        "    seq_len = tgt.size(1)\n",
        "    tgt_mask = torch.tril(torch.ones((1, seq_len, seq_len), device=tgt.device)).bool()\n",
        "    return src_mask, tgt_mask\n",
        "\n",
        "# ----------------------------\n",
        "# Model + Training Setup\n",
        "# ----------------------------\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vocab_size = sp.get_piece_size()\n",
        "\n",
        "# Use the Transformer class already defined in Cell 2\n",
        "model = Transformer(vocab_size=vocab_size)\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
        "\n",
        "num_epochs = 25\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "print(f\"Training setup complete on {device}\")\n",
        "print(\"⏭ Training skipped. Loading pretrained model instead...\")\n",
        "\n",
        "# ----------------------------\n",
        "# Load pretrained model\n",
        "# ----------------------------\n",
        "model.load_state_dict(torch.load(\"best_masked_model.pt\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "print(\"Model loaded successfully and ready for inference!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLrL4EwGTMjb",
        "outputId": "216fac63-0b9c-4e37-b606-6b4e543b781d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test data loaded — Samples: 1961\n",
            "\n",
            "Calculating Metrics...\n",
            "\n",
            "=== EVALUATION RESULTS ===\n",
            "BLEU Score:     16.04\n",
            "ROUGE-L Score:  0.00\n",
            "chrF Score:     26.24\n",
            "Perplexity:     12302.73\n",
            "\n",
            "=== QUALITATIVE EXAMPLES ===\n",
            "\n",
            "Example 1\n",
            "Reference: اس جمود کو توڑا ہے۔\n",
            "Generated: ہر چیز انتہایی زبردست اور اعلی معیار کی تہی\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 2\n",
            "Reference: انہوںنے ناریل کے چہلکے سے کاربن حاصل کرکے\n",
            "Generated: تو اس کو جمود ہے۔\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 3\n",
            "Reference: اسے پتہ بہی ہے کہ وہ جہوٹ بول رہا ہے\n",
            "Generated: انہوںنے ناریل کے چہلکے سے کاربن حاصل کرکے\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 4\n",
            "Reference: افسوس، یہ ہے۔\n",
            "Generated: اسے وہ بہی بے حسی دیا ہے کہ رہا ہے\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 5\n",
            "Reference: اوراس کے ساتہ اداروں میں تصادم کو بہی روکنا ہے۔\n",
            "Generated: یہ افسوس، یہ ہے۔\n",
            "------------------------------------------------------------\n",
            "Results saved to evaluation_results.json\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pickle\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu\n",
        "import json\n",
        "\n",
        "PAD_ID = 0\n",
        "SOS_ID = 2\n",
        "EOS_ID = 3\n",
        "\n",
        "# ----------------------------\n",
        "# Dataset for next-sentence prediction\n",
        "# ----------------------------\n",
        "class UrduDataset(Dataset):\n",
        "    def __init__(self, tokenized_sentences, max_len=50):\n",
        "        self.data = []\n",
        "        self.max_len = max_len\n",
        "\n",
        "        for i in range(len(tokenized_sentences) - 1):\n",
        "            src_tokens = tokenized_sentences[i]\n",
        "            tgt_tokens = tokenized_sentences[i + 1]\n",
        "\n",
        "            if len(src_tokens) < 3 or len(tgt_tokens) < 3:\n",
        "                continue\n",
        "\n",
        "            src = [SOS_ID] + src_tokens + [EOS_ID]\n",
        "            tgt = [SOS_ID] + tgt_tokens + [EOS_ID]\n",
        "\n",
        "            if len(src) <= max_len and len(tgt) <= max_len:\n",
        "                self.data.append((src, tgt))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch = [b[0] for b in batch]\n",
        "    tgt_batch = [b[1] for b in batch]\n",
        "\n",
        "    max_src = max(len(s) for s in src_batch)\n",
        "    max_tgt = max(len(t) for t in tgt_batch)\n",
        "\n",
        "    src_padded = [s + [PAD_ID] * (max_src - len(s)) for s in src_batch]\n",
        "    tgt_padded = [t + [PAD_ID] * (max_tgt - len(t)) for t in tgt_batch]\n",
        "\n",
        "    return torch.LongTensor(src_padded), torch.LongTensor(tgt_padded)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Load test set\n",
        "# ----------------------------\n",
        "with open('test_tokenized.pkl', 'rb') as f:\n",
        "    test_data = pickle.load(f)\n",
        "\n",
        "test_dataset = UrduDataset(test_data, max_len=50)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(f\"Test data loaded — Samples: {len(test_dataset)}\")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Evaluation Function\n",
        "# ----------------------------\n",
        "def evaluate_model(model, dataloader, sp, device):\n",
        "    model.eval()\n",
        "    references, hypotheses = [], []\n",
        "\n",
        "    total_loss = 0\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in dataloader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            src_mask, tgt_mask = create_masks(src, tgt_input)\n",
        "\n",
        "            output = model(src, tgt_input, src_mask, tgt_mask)\n",
        "\n",
        "            loss = criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # ----------------------------\n",
        "            # Greedy decoding\n",
        "            # ----------------------------\n",
        "            for i in range(src.size(0)):\n",
        "                src_single = src[i:i+1]\n",
        "                generated = [SOS_ID]\n",
        "\n",
        "                for _ in range(50):  # max length\n",
        "                    tgt_tensor = torch.LongTensor([generated]).to(device)\n",
        "\n",
        "                    src_mask, tgt_mask = create_masks(src_single, tgt_tensor)\n",
        "                    out = model(src_single, tgt_tensor, src_mask, tgt_mask)\n",
        "\n",
        "                    next_tok = out[0, -1, :].argmax().item()\n",
        "\n",
        "                    if next_tok == EOS_ID or next_tok == PAD_ID:\n",
        "                        break\n",
        "\n",
        "                    generated.append(next_tok)\n",
        "\n",
        "                # Decode text\n",
        "                hypothesis = sp.decode_ids(generated[1:])\n",
        "                reference_tokens = [t for t in tgt[i].cpu().tolist() if t not in [PAD_ID, SOS_ID, EOS_ID]]\n",
        "                reference = sp.decode_ids(reference_tokens)\n",
        "\n",
        "                hypotheses.append(hypothesis)\n",
        "                references.append([reference])  # sacrebleu format\n",
        "\n",
        "    print(\"\\nCalculating Metrics...\")\n",
        "\n",
        "    # BLEU & chrF\n",
        "    bleu = sacrebleu.corpus_bleu(hypotheses, references)\n",
        "    chrf = sacrebleu.corpus_chrf(hypotheses, references)\n",
        "\n",
        "    # ROUGE-L\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=False)\n",
        "    rouge_vals = [\n",
        "        scorer.score(ref[0], hyp)['rougeL'].fmeasure\n",
        "        for hyp, ref in zip(hypotheses, references)\n",
        "    ]\n",
        "    rouge_l = np.mean(rouge_vals) * 100\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    perplexity = np.exp(avg_loss)\n",
        "\n",
        "    return {\n",
        "        'bleu': bleu.score,\n",
        "        'rouge_l': rouge_l,\n",
        "        'chrf': chrf.score,\n",
        "        'perplexity': perplexity,\n",
        "        'hypotheses': hypotheses[:5],\n",
        "        'references': [r[0] for r in references[:5]]\n",
        "    }\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Run Evaluation\n",
        "# ----------------------------\n",
        "results = evaluate_model(model, test_loader, sp, device)\n",
        "\n",
        "print(\"\\n=== EVALUATION RESULTS ===\")\n",
        "print(f\"BLEU Score:     {results['bleu']:.2f}\")\n",
        "print(f\"ROUGE-L Score:  {results['rouge_l']:.2f}\")\n",
        "print(f\"chrF Score:     {results['chrf']:.2f}\")\n",
        "print(f\"Perplexity:     {results['perplexity']:.2f}\")\n",
        "\n",
        "print(\"\\n=== QUALITATIVE EXAMPLES ===\")\n",
        "for i in range(len(results['hypotheses'])):\n",
        "    print(f\"\\nExample {i+1}\")\n",
        "    print(f\"Reference: {results['references'][i]}\")\n",
        "    print(f\"Generated: {results['hypotheses'][i]}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# ----------------------------\n",
        "# Save results\n",
        "# ----------------------------\n",
        "to_save = {\n",
        "    'bleu': results['bleu'],\n",
        "    'rouge_l': results['rouge_l'],\n",
        "    'chrf': results['chrf'],\n",
        "    'perplexity': results['perplexity'],\n",
        "    'examples': [\n",
        "        {'reference': ref, 'generated': hyp}\n",
        "        for ref, hyp in zip(results['references'], results['hypotheses'])\n",
        "    ]\n",
        "}\n",
        "\n",
        "with open('evaluation_results.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(to_save, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Results saved to evaluation_results.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "Ds9aRyLZUEtD",
        "outputId": "fbc37c12-4fad-473e-d773-cbf1c482ad00"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-16 17:02:33.640 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.644 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.645 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.647 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.648 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.650 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.652 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.654 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.655 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.657 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.658 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.659 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.661 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.663 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.665 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.666 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.668 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.669 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.671 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.673 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.675 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.677 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.680 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.681 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.682 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.683 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.685 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.686 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.688 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.690 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.692 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.694 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.695 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.696 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.698 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-16 17:02:33.701 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ],
      "source": [
        "import streamlit as st\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import sentencepiece as spm\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Load model + tokenizer ONLY ONCE\n",
        "# -------------------------------------------------------\n",
        "@st.cache_resource\n",
        "def load_model_and_tokenizer():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    sp = spm.SentencePieceProcessor()\n",
        "    sp.load(\"urdu_tokenizer.model\")\n",
        "\n",
        "    # Load model (Transformer class from Cell 2)\n",
        "    model = Transformer(vocab_size=sp.vocab_size())\n",
        "    model.load_state_dict(torch.load(\"best_masked_model.pt\", map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    return model, sp, device\n",
        "\n",
        "model, sp, device = load_model_and_tokenizer()\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Decoding Functions\n",
        "# -------------------------------------------------------\n",
        "def greedy_decode(model, src, sp, device, max_len=50):\n",
        "    model.eval()\n",
        "    src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_output = model.encoder_embedding(src) * (model.d_model ** 0.5)\n",
        "        enc_output = model.positional_encoding(enc_output)\n",
        "        for layer in model.encoder_layers:\n",
        "            enc_output = layer(enc_output, src_mask)\n",
        "\n",
        "    ys = torch.ones(1, 1, dtype=torch.long, device=device) * 2  # BOS token\n",
        "\n",
        "    for _ in range(max_len - 1):\n",
        "        tgt_mask = torch.tril(torch.ones((1, ys.size(1), ys.size(1)), device=device)).bool()\n",
        "        out = model.decoder_embedding(ys) * (model.d_model ** 0.5)\n",
        "        out = model.positional_encoding(out)\n",
        "        dec_output = out\n",
        "        for layer in model.decoder_layers:\n",
        "            dec_output = layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "        logits = model.fc_out(dec_output[:, -1])\n",
        "        next_token = torch.argmax(logits, dim=-1).item()\n",
        "        if next_token == 3:  # EOS token\n",
        "            break\n",
        "        ys = torch.cat([ys, torch.tensor([[next_token]], device=device)], dim=1)\n",
        "    return sp.decode_ids(ys.squeeze().tolist()[1:])\n",
        "\n",
        "def beam_search_decode(model, src, sp, device, beam_width=3, max_len=50):\n",
        "    model.eval()\n",
        "    src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_output = model.encoder_embedding(src) * (model.d_model ** 0.5)\n",
        "        enc_output = model.positional_encoding(enc_output)\n",
        "        for layer in model.encoder_layers:\n",
        "            enc_output = layer(enc_output, src_mask)\n",
        "\n",
        "    beams = [(torch.tensor([[2]], device=device), 0)]  # (sequence, score)\n",
        "    for _ in range(max_len):\n",
        "        new_beams = []\n",
        "        for seq, score in beams:\n",
        "            tgt_mask = torch.tril(torch.ones((1, seq.size(1), seq.size(1)), device=device)).bool()\n",
        "            out = model.decoder_embedding(seq) * (model.d_model ** 0.5)\n",
        "            out = model.positional_encoding(out)\n",
        "            dec_output = out\n",
        "            for layer in model.decoder_layers:\n",
        "                dec_output = layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "            logits = model.fc_out(dec_output[:, -1])\n",
        "            probs = F.log_softmax(logits, dim=-1).squeeze(0)\n",
        "            topk = torch.topk(probs, beam_width)\n",
        "            for i in range(beam_width):\n",
        "                next_tok = topk.indices[i].item()\n",
        "                next_score = score + topk.values[i].item()\n",
        "                new_seq = torch.cat([seq, torch.tensor([[next_tok]], device=device)], dim=1)\n",
        "                new_beams.append((new_seq, next_score))\n",
        "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "        if all(seq[0, -1].item() == 3 for seq, _ in beams):\n",
        "            break\n",
        "    best_seq = beams[0][0].squeeze().tolist()\n",
        "    best_seq = [t for t in best_seq[1:] if t not in [0, 3]]\n",
        "    return sp.decode_ids(best_seq)\n",
        "\n",
        "def top_k_sampling(logits, k=50, temperature=0.9):\n",
        "    logits = logits / temperature\n",
        "    values, indices = torch.topk(logits, k)\n",
        "    probs = F.softmax(values, dim=-1)\n",
        "    return indices[torch.multinomial(probs, 1)].item()\n",
        "\n",
        "def top_k_decode(model, src, sp, device, k=50, temperature=0.9, max_len=50):\n",
        "    model.eval()\n",
        "    src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_output = model.encoder_embedding(src) * (model.d_model ** 0.5)\n",
        "        enc_output = model.positional_encoding(enc_output)\n",
        "        for layer in model.encoder_layers:\n",
        "            enc_output = layer(enc_output, src_mask)\n",
        "\n",
        "    ys = torch.ones(1, 1, dtype=torch.long, device=device) * 2\n",
        "    for _ in range(max_len - 1):\n",
        "        tgt_mask = torch.tril(torch.ones((1, ys.size(1), ys.size(1)), device=device)).bool()\n",
        "        out = model.decoder_embedding(ys) * (model.d_model ** 0.5)\n",
        "        out = model.positional_encoding(out)\n",
        "        dec_output = out\n",
        "        for layer in model.decoder_layers:\n",
        "            dec_output = layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "        logits = model.fc_out(dec_output[:, -1])\n",
        "        next_token = top_k_sampling(logits.squeeze(), k=k, temperature=temperature)\n",
        "        if next_token == 3:\n",
        "            break\n",
        "        ys = torch.cat([ys, torch.tensor([[next_token]], device=device)], dim=1)\n",
        "    return sp.decode_ids(ys.squeeze().tolist()[1:])\n",
        "\n",
        "def generate_response(model, src_text, sp, device, decoding=\"Greedy\", max_len=50):\n",
        "    src_tokens = [2] + sp.encode_as_ids(src_text) + [3]\n",
        "    src = torch.LongTensor([src_tokens]).to(device)\n",
        "    if decoding == \"Greedy\":\n",
        "        return greedy_decode(model, src, sp, device, max_len)\n",
        "    elif decoding == \"Beam Search\":\n",
        "        return beam_search_decode(model, src, sp, device, max_len=max_len)\n",
        "    elif decoding == \"Top-k Sampling\":\n",
        "        return top_k_decode(model, src, sp, device, max_len=max_len)\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Streamlit UI\n",
        "# -------------------------------------------------------\n",
        "st.set_page_config(page_title=\"Urdu Chatbot\", layout=\"wide\")\n",
        "st.markdown(\"\"\"\n",
        "## 🇵🇰 Urdu Chatbot — Transformer Encoder-Decoder  \n",
        "### **Decoding: Greedy | Beam Search | Top-k Sampling**  \n",
        "Right-to-left Urdu text supported.\n",
        "\"\"\")\n",
        "\n",
        "# Maintain conversation history\n",
        "if \"history\" not in st.session_state:\n",
        "    st.session_state.history = []\n",
        "\n",
        "col1, col2 = st.columns(2)\n",
        "\n",
        "with col1:\n",
        "    user_input = st.text_area(\n",
        "        \"📝 پیغام لکھیں\",\n",
        "        placeholder=\"یہاں لکھیں...\",\n",
        "        height=120\n",
        "    )\n",
        "    decoding_strategy = st.radio(\n",
        "        \"Decoding Strategy\",\n",
        "        [\"Greedy\", \"Beam Search\", \"Top-k Sampling\"],\n",
        "        index=2\n",
        "    )\n",
        "    if st.button(\"Send ➤\"):\n",
        "        if user_input.strip():\n",
        "            reply = generate_response(model, user_input, sp, device, decoding_strategy)\n",
        "            st.session_state.history.append(f\"آپ: {user_input}\")\n",
        "            st.session_state.history.append(f\"بوٹ: {reply}\")\n",
        "            st.success(reply)\n",
        "        else:\n",
        "            st.error(\"براہ کرم کوئی پیغام لکھیں۔\")\n",
        "\n",
        "with col2:\n",
        "    st.text_area(\n",
        "        \"گفتگو کا خلاصہ\",\n",
        "        value=\"\\n\".join(st.session_state.history),\n",
        "        height=380\n",
        "    )\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
